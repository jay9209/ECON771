---
title: "Assignment2"
author: "Jung Jae Kim"
output: pdf_document
always_allow_html: true
extra_dependencies:
  amsmath: null
---

```{r setup, include=FALSE, cache=TRUE, tidy=TRUE, echo=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(readr, tidyverse, vroom, here, sqldf, ggthemes, fixest, modelsummary, plm, GGally, ivreg, ivmodel, kableExtra, robomit, xtable)
knitr::opts_chunk$set(echo = TRUE)
```


```{r import-data, warning=FALSE, include=FALSE}
dat <- read_csv("/Users/jjk84/ECON771/dat.csv")
price.shock <- read_csv("/Users/jjk84/ECON771/price.shock.sum.csv")
```



Questions

1.  Provide and discuss a table of simple summary statistics showing the mean, standard deviation, min, and max of total physician-level Medicare spending, claims, and patients. Use the Medicare utilization and payment data to calculate total spending, claims, and patients at the physician level. You can do this using the average Medicare allowed amt * bene_day_srvc_cnt (or Medicare allowed amt * line_srvc_cnt) for spending, bene_day_srvc_cnt or the line_srvc_cnt for claims, and bene_unique_cnt for patients. The patient counts will include some overlap since the data are by service, but that's OK for our purposes.


```{r Q1, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
dat %>% 
  ungroup() %>%
  summarise_at(c('Total_Spending', 'Total_Claims', 'Total_Patients'),
               list(Mean = mean, Std.Dev. = sd, Min = min, Max = max), na.rm=T) %>%
  pivot_longer(cols = everything(),
               names_to = c("Names", ".value"), 
               names_sep = "_",
               names_prefix = "Total_") -> table1
```
```{r summary-stats-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
table1 %>% kbl() %>% kable_styling(position = "center",full_width=TRUE)
```

\newpage
2.Form a proxy for integration using the ratio:
\begin{equation}
INT_{it} = \mathbf{1} \left(\frac{HOPD_{it}}{HOPD_{it} + OFFICE_{it} + ASC_{it}} \geq 0.75\right),
\end{equation}
where $HOPD_{it}$ reflects the total number of claims in which physician $i$ bills in a hospital outpatient setting, $OFFICE_{it}$ is the total number of claims billed to an office setting, and $ASC_{it}$ is the total number of claims billed to an ambulatory surgery center. As reflected in Equation (1), you can assume that any physician with at least 75% of claims billed in an outpatient setting is integrated with a hospital. Using this 75% threshold, plot the mean of total physician-level claims for integrated versus non-integrated physicians over time.

```{r Q2, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
dat  %>%
  filter(!is.na(int)) %>%
  mutate(int = as.factor(int)) %>%
  group_by(Year, int) %>%
  summarise(mean_claims_count = mean(Total_Claims, na.rm = TRUE)) %>%
  ggplot(aes(y=mean_claims_count , x=Year, 
             group=int, color=int)) +
  geom_line(size=0.8) +
  theme_bw()+ 
  labs(x="Years", y="Number of Claims", 
       title = "Mean of total physician-level claims for integrated vs non-integrated physicians")+
  theme(plot.title = element_text(hjust = 0.5)) -> plot1
```

```{r plot-summary-stats-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
plot1
```

\newpage
3.Estimate the relationship between integration on (log) total physician claims using OLS, with the following specification:
\begin{equation}
y_{it} = \delta INT_{it} + \beta x_{it} + \gamma_{i} + \gamma_{t} + \varepsilon_{it}, 
\end{equation}
where $INT_{it}$ is defined in Equation (1), $x_{it}$ captures time-varying physician characteristics, and $\gamma_{i}$ and $\gamma_{t}$ denote physician and time fixed effects. Please focus on physician's that weren't yet integrated as of 2012, that way we have some pre-integration data for everyone. Impose this restriction for the remaining questions. Feel free to experiment with different covariates in $x_{it}$ or simply omit that term and only include the fixed effects.

```{r Q3, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
npi_exclude_list <- dat[which(dat$Year==2012 & dat$int==1),]$npi
reg.dat <- dat %>% 
  filter(!is.na(int)) %>%
  filter(!npi %in% npi_exclude_list) %>%
  select(c("Year", "npi", "Total_Claims", "int", "group1")) %>%
  mutate(log_claim = log(Total_Claims)) 

mod.fe <- feols(log_claim ~ int | npi + Year, dat = reg.dat)
```
```{r Q3-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
etable(mod.fe) %>% kbl() %>% kable_styling(position = "center",full_width=TRUE)
```
\newpage
4.How much should we be "worried" about endogeneity here? Extending the work of @altonji2005, @oster2019 derives the expression
\begin{equation}
\delta^{*} \approx \hat{\delta}_{D,x_{1}} - \rho \times \left[\hat{\delta}_{D} - \hat{\delta}_{D,x_{1}}\right] \times \frac{R_{max}^{2} - R_{D,x_{1}}^{2}}{R_{D,x_{1}}^{2} - R_{D}^{2}} \xrightarrow{p} \delta,
\end{equation}
where $x_{1}$ captures our observable covariates (or fixed effects in our case); $\delta$ denotes the treatment effect of interest; $\hat{\delta}_{D,x_{1}}$ denotes the coefficient on $D$ from a regression of $y$ on $D$ and $x_{1}$; $R_{D,x_{1}}^{2}$ denotes the $R^{2}$ from that regression; $\hat{\delta}_{D}$ denotes the coefficient on $D$ from a regression of $y$ on $D$ only; $R_{D}^{2}$ reflects the $R^{2}$ from that regression; $R_{max}^{2}$ denotes an unobserved "maximum" $R^{2}$ from a regression of $y$ on $D$, observed covariates $x_{1}$, and some unobserved covariates $x_{2}$; and $\rho$ denotes the degree of selection on observed variables relative to unobserved variables. One approach that Oster suggests is to consider a range of $R^{2}_{max}$ and $\rho$ to bound the estimated treatment effect, where the bounds are given by $\left[ \hat{\delta}_{D,x_{1}}, \delta^{*}(R^{2}_{max}, \rho) \right]$. Construct these bounds based on all combinations of $\rho \in (0, .5, 1, 1.5, 2)$ and $R_{max}^{2} \in (0.5, 0.6, 0.7, 0.8, 0.9, 1)$ and present your results in a table. What do your results say about the extent to which selection on observables could be problematic here? Hint: you can also look into `psacalc` in `Stata` or `robomit` in `R` for implementation of @oster2019 in `Stata` or `R`, respectively.


```{r Q4, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
mod.ols <- lm(log_claim ~ int, dat = reg.dat)
oster <- function(r_max,rho){
  delta_fe <- as.numeric(coef(mod.fe))
  delta_ols <- as.numeric(coef(mod.ols)[2])
  r_fe <-  as.numeric(r2(mod.fe)[2])
  r_ols <- summary(mod.ols)$r.squared
  
  delta_oster <- delta_fe - rho * (delta_ols - delta_fe) * (r_max - r_fe) / (r_fe - r_ols)
  print(c(delta_fe,delta_oster))
}

rho_list <- seq(0, 2, 0.5)
R2max_list <- c(0.95,1)
tab <- data.frame()
for (i in 1:length(rho_list)){
  row <- data.frame()
  for (j in 1:length(R2max_list)){
    rho <- rho_list[i]
    R2max <- R2max_list[j]
    interval <- paste0('[',round(oster(R2max,rho)[1],6),',',round(oster(R2max,rho)[2],6),']')
    colname <- paste0('R2max=',R2max)
    row[1,colname] <- interval
  }
  tab <- bind_rows(tab, row)
}
rownames(tab) <- c('rho=0','rho=0.5','rho=1','rho=1.5',
                   'rho=2')
```
```{r Q4-disply, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
tab %>% kbl() %>% kable_styling(position = "center",full_width=TRUE)
```
The selection on unobservable could not be problematic here since the bound is not too big.


5. Construct the change in Medicare payments achievable for an integrated versus non-integrated physician practice due to the 2010 update to the physician fee schedule, $\Delta P_{it}$. Use this as an instrument for $INT_{it}$ in a 2SLS estimator following the same specification as in Equation (2). Present your results along with those of your "first stage" and "reduced form".


```{r Q5, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
##### Differential timgin treatment
reg.dat.iv <- reg.dat %>%
  mutate(npi = as.character(npi)) %>%
  rename(year = Year) %>%
  rename(tax_id = group1) %>%
  inner_join(price.shock,by=c("tax_id","year"))

first_stage <- feols(int ~ practice_rev_change | npi + year, dat = reg.dat.iv)
reg.dat.iv <- reg.dat.iv %>%
  mutate(fitted_int = fitted(first_stage)) %>%
  mutate(resid = first_stage$residuals)
second_stage <- feols(log_claim ~ fitted_int | npi + year, dat = reg.dat.iv )
```
```{r sa-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
etable(first_stage,second_stage) %>% kbl() %>% kable_styling(position = "center",full_width=TRUE)
```


\newpage
6.Assess the "need" for IV by implementing a Durbin-Wu-Hausman test with an augmented regression. Do this by first estimating the regression, $INT_{it} = \lambda \Delta P_{it} + \beta x_{it} + \gamma_{i} + \gamma_{t} + \varepsilon_{it}$, take the residual $\hat{\nu} = INT_{it} - \hat{INT}_{it}$, and run the regression $$y_{it} = \delta INT_{it} + \beta x_{it} + \gamma_{i} + \gamma_{t} + \kappa \hat{\nu} + \varepsilon_{it}.$$ Discuss your results for $\hat{\kappa}$.


``` {r Q6, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
durbin_wu_hausman <- feols(log_claim ~ int + resid | npi + year, dat = reg.dat.iv)
```
```{r Q6-plot-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
etable(durbin_wu_hausman) %>% kbl() %>% kable_styling(position = "center",full_width=TRUE)
```
Since the coefficien of residuals is significantly different from 0, we need IV.


7.Now let's pay attention to potential issues of weak instruments. As we discussed in class, one issue with weak instruments is that our typical critical values (say, 1.96 for a 95% confidence interval) from the equation of interest (sometimes called the structural equation) are too low in the presence of a weak first-stage. These issues are presented very clearly and more formally in the Andrews, Stock, and Sun (2019) survey article. For this question, you will consider two forms of inference in the presence of weak instruments. Hint: Check out the `ivmodel` package in R or the `ivreg2` command in Stata for help getting the AR Wald statistic.
    - Present the results of a test of the null, $H_{0}: \delta=0$, using the Anderson-Rubin Wald statistic. Do your conclusions from this test differ from a traditional t-test following 2SLS estimation of Equation (2)?
    - Going back to your 2SLS results...inflate your 2SLS standard errors to form the $tF$ adjusted standard error, following Table 3 in @lee2021. Repeat the test of the null, $H_{0}: \delta=0$, using standard critical values and the $tF$ adjusted standard error.
    

```{r Q7, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
reg.dat.iv <- reg.dat.iv %>%
  mutate(log_claim_FWL = feols(log_claim ~ 1 | npi + year, dat = reg.dat.iv)$residuals,
         int_FWL = feols(int ~ 1 | npi + year, dat = reg.dat.iv)$residuals,
         practice_rev_change_FWL = feols(practice_rev_change ~ 1 | npi + year, dat = reg.dat.iv)$residuals)

anderson_rubin <- AR.test(ivmodelFormula(log_claim_FWL ~ int_FWL | practice_rev_change_FWL, dat = reg.dat.iv))
as.numeric(anderson_rubin$ci)


first_stage_F = as.numeric((first_stage$coefficients / first_stage$se)^2)
print(paste0('First stage F is ',round(first_stage_F)))

# Since the first stage F-statistics is bigger than 104.67, the adjustment factor is 1.
tF = 1
lower <- second_stage$coefficients[['fitted_int']]-1.96*second_stage$se[['fitted_int']]*tF
upper <- second_stage$coefficients[['fitted_int']]+1.96*second_stage$se[['fitted_int']]*tF


```
```{r Q7-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
print(paste0('Anderson-Rubin confidence interval is  ','[',round(as.numeric(anderson_rubin$ci),4)[1],',',
             round(as.numeric(anderson_rubin$ci),4)[2],']'))
print(paste0('AtF adjusted confidence interval is  ','[',round(lower,4),',',
             round(upper,4),']'))
```

Since the F-statistics of the first stage regression is sufficiently large, the results are similar.

\newpage
8.Following the Borusyak and Hull (2021) working paper (BH), we can consider our instrument as a function of some exogenous policy shocks and some possibly endogenous physician characteristics, $\Delta P_{it}=f\left(g_{pt}; z_{ipt}\right)$, where $g_{pt}$ captures overall payment shocks for procedure $p$ at time $t$, and $z_{ip}$ denotes a physician's quantity of different procedures at baseline. We can implement the BH re-centering approach as follows:
    - Consider hypothetical price changes over a set of possible counterfactuals by assuming that the counterfactuals consist of different allocations of the observed relative price changes. For example, take the vector of all relative price changes, reallocate this vector randomly, and assign new hypothetical relative price changes. Do this 100 times. This isn't "all" possible counterfactuals by any means, but it will be fine for our purposes.
    - Construct the expected revenue change over all possible realizations from previously, $\mu_{it} = E [\Delta P_{it}]= \sum_{s=1}^{100} \sum_{p} g_{pt}^{s} z_{ip}$.
    - Re-estimate Equation (2) by 2SLS when instrumenting for $INT_{it}$ with $\tilde{\Delta} P_{it} = \Delta P_{it} - \mu_{it}$. Intuitively, this re-centering should isolate variation in the instrument that is only due to the policy and remove variation in our instrument that is due to physician practice styles (the latter of which is not a great instrument).
    


```{r Q8, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, include=FALSE, warning=FALSE}
pseudoIV <- read_delim("/Users/jjk84/ECON771/pseudoIV.csv", delim = "\t", 
    escape_double = FALSE, trim_ws = TRUE)

pseudoIV <- pseudoIV %>%
  rename(year = Year)

reg.dat.iv.bh <- reg.dat.iv %>%
  left_join(pseudoIV,by=c("tax_id","year")) %>%
  mutate(recenter_rev_change = practice_rev_change - mu)
  
iv_fixed_bh <- feols(log_claim ~ 1 | npi + year | int ~ recenter_rev_change, dat = reg.dat.iv.bh)
```


```{r Q8-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
etable(iv_fixed_bh) %>% kbl() %>% kable_styling(position = "center",full_width=TRUE)
```


9. Discuss your findings and compare estimates from different estimators.

Answers: Although the effect of integration on claims is consistently negative, there is a huge difference in terms of magnitude of estimates from FE and IV regression results. Moreover, BH result indicates that we need to use instruments with only exogenous policy shocks. 


10. Reflect on this assignment. What did you find most challenging? What did you find most surprising? 

Answers: It was very helpful to conduct several assumption check for IV model. It would be very interesting if I had a weak instrument, which is not the case in this exercise, and did same exercise. 
