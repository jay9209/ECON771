---
title: "Assignment1"
author: "Jung Jae Kim"
output: pdf_document
header-includes:
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
always_allow_html: true
extra_dependencies:
  amsmath: null
---

```{r setup, include=FALSE, cache=TRUE, tidy=TRUE, echo=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, ggplot2, dplyr, lubridate, haven, descriptr, HonestDiD, Rglpk, did, fixest, RColorBrewer, patchwork, ggthemes, DT, plotly, here, crosswalkr, plm, stargazer, modelsummary, fixest, DRDID, ggthemes, patchwork, devtools, lfe, kableExtra)
knitr::opts_chunk$set(echo = TRUE)
```


```{r import-data, include=FALSE, warning=FALSE}
data_hcris <- read.delim("/Users/jjk84/ECON771/HCRIS_Data.txt")
data_pos <- read_delim("/Users/jjk84/ECON771/pos-data-combined.txt")
data_aca<- read.delim("/Users/jjk84/ECON771/medicaid_expansion.txt")
```

```{r merge-1, include=FALSE, warning=FALSE }
# Focus on the years from 2003 through 2019
data_hcris <- data_hcris %>% filter(year>=2003 & year<=2019)
data_pos <- data_pos %>% filter(year>=2003 & year<=2019)

# Adjust dollar unit to the million
data_hcris <- data_hcris %>%
  mutate(provider = provider_number) %>%
  mutate(uncomp_care = uncomp_care / 1000000) %>%
  mutate(tot_rev = tot_pat_rev / 1000000)

# Change the state name to abbreviation
data_aca <- data_aca %>%
  mutate(state = c(state.abb,'DC')[match(State,
                                         c(state.name,'District of Columbia'))],
         year_adopted = format(as.Date(data_aca$date_adopted), format="%Y")) %>%
  select(state, year_adopted, expanded)

data_merged <- data_hcris %>% 
  mutate(provider=as.character(provider),
         year=as.double(year)) %>% 
  inner_join(data_pos, by=c('provider','year')) %>%
  mutate(year=as.numeric(year),
         state = state.x) %>%
  left_join(data_aca, by='state')
```

\blandscape

Questions

1. Provide and discuss a table of simple summary statistics showing the mean, standard deviation, min, and max of hospital total revenues and uncompensated care over time.

```{r summary-stats, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
table1 <- data_merged %>%
  group_by(year) %>%
  summarise_at(c('tot_rev', 'uncomp_care'),
               list(mean = mean, sd = sd, min = min, max = max), na.rm=T) %>%
  relocate("year",starts_with("tot"), starts_with("unc"))
```
```{r summary-stats-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
table1 %>% kbl()
```


In 2016, the max value of uncompensated care is $20404 mil, which is 10 times bigger than max values in other periods. This might be reporting error, so I delete this observation for future analysis.

```{r remove-outlier, include=FALSE, warning=FALSE}
data_merged <- data_merged %>%
  filter(uncomp_care < 20000)
```

\elandscape

\newpage
2. Create a figure showing the mean hospital uncompensated care from 2003 to 2019. Show this trend separately by hospital ownership type (private not for profit and private for profit).
```{r plot-summary-stats, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
plot1 <- ggplot(table1, aes(x = year, y = uncomp_care_mean))+
  theme(plot.title = element_text(size=12)) +
  geom_line(size=1) +
  labs(title = "Hospital Uncompensated Care Over Time9",
       x = "Year",
       y = "Uncompensated Care")

plot2 <- data_merged %>%
  filter((own_type == "Non-profit Private" | own_type == "Profit")) %>%
  group_by(year,own_type) %>%
  summarise_at(c('uncomp_care'),list(uncomp_care_mean=mean),na.rm=T) %>%
  ggplot(aes(x=year, y=uncomp_care_mean, color=own_type)) +
  geom_line(size = 0.8) +
  guides(color=guide_legend(title="Ownership")) +
  theme_bw()+
  labs(x="Years", y="Mean Uncompensated Care", 
       title = "Mean of Hospital Uncompensated Care by Ownership Type", 
       fill = "Ownership type", color = "Ownership type")
```

```{r plot-summary-stats-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
plot1
plot2
```
\newpage
3. Using a simple DD identification strategy, estimate the effect of Medicaid expansion on hospital uncompensated care using a traditional two-way fixed effects (TWFE) estimation:
\begin{equation}
y_{it} = \alpha_{i} + \gamma_{t} + \delta D_{it} + \varepsilon_{it},
\end{equation}
where $D_{it}=1(E_{i}\leq t)$ in Equation \@ref(eq:dd) is an indicator set to 1 when a hospital is in a state that expanded as of year $t$ or earlier, $\gamma_{t}$ denotes time fixed effects, $\alpha_{i}$ denotes hospital fixed effects, and $y_{it}$ denotes the hospital's amount of uncompensated care in year $t$. Present four estimates from this estimation in a table: one based on the full sample (regardless of treatment timing); one when limiting to the 2014 treatment group (with never treated as the control group); one when limiting to the 2015 treatment group (with never treated as the control group); and one when limiting to the 2016 treatment group (with never treated as the control group). Briefly explain any differences.

```{r DiD, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
result = list()
for(i in 1:4){
  if (i == 1){
    data_DiD <- data_merged %>%
      filter(!is.na(uncomp_care)) %>%
      select(provider,year,state,uncomp_care,expanded,year_adopted) %>%
      mutate(treatment = ifelse(expanded == TRUE & year >= year_adopted, 1, 0))
    result[[i]] = felm(formula = uncomp_care ~ treatment | provider + year | 0 | provider,
                       data = data_DiD)
  }
    
  else{
  data_DiD <- data_merged %>%
    filter(!is.na(uncomp_care)) %>%
    filter(expanded == FALSE | year_adopted==i+2012) %>%
    select(provider,year,state,uncomp_care,expanded,year_adopted) %>%
    mutate(treatment = ifelse(expanded == TRUE & year >= year_adopted, 1, 0))
  result[[i]] = felm(formula = uncomp_care ~ treatment | provider + year | 0 | provider,
                     data = data_DiD)
  }
}

```
```{r DiD-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
modelsummary(result,stars=TRUE,output='markdown', note="(1)-(4) representes full, 2014 group, 2015 group and 2016 group respectevely")
```
\newpage
4. Estimate an "event study" version of the specification in part 3:
\begin{equation}
y_{it} = \alpha_{i} + \gamma_{t} +\sum_{\tau < -1} D_{it}^{\tau} \delta_{\tau} + \sum_{\tau>=0} D_{it}^{\tau} \delta_{\tau} + \varepsilon_{it},
\end{equation}
where $D_{it}^{\tau} = 1(t-E_{i}=\tau)$ in Equation \@ref(eq:event) is essentially an interaction between the treatment dummy and a relative time dummy. In this notation and context, $\tau$ denotes years relative to Medicaid expansion, so that $\tau=-1$ denotes the year before a state expanded Medicaid, $\tau=0$ denotes the year of expansion, etc. Estimate with two different samples: one based on the full sample and one based only on those that expanded in 2014 (with never treated as the control group).


```{r event-study, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
result_event = list()
for(i in 1:2){
  if (i==1){
    data_event <- data_merged %>%
      filter(!is.na(uncomp_care)) %>%
      select(provider,year,state,uncomp_care,expanded,year_adopted) %>%
      mutate(relative_year = ifelse(expanded == FALSE, 0 ,year-as.numeric(year_adopted)),
             relative_year = ifelse(relative_year < -5, -5, relative_year),
             treated_ever = ifelse(expanded == TRUE, 1, 0))
    result_event[[i]] = feols(uncomp_care~i(relative_year,treated_ever,ref=-1) | provider+year,
                              cluster=~provider, data=data_event)
  }
  
  else{
    data_event <- data_merged %>%
      filter(!is.na(uncomp_care)) %>%
      filter(expanded == FALSE | year_adopted==i+2012) %>%
      select(provider,year,state,uncomp_care,expanded,year_adopted) %>%
      mutate(relative_year = ifelse(expanded == FALSE, 0 ,year-as.numeric(year_adopted)),
             relative_year = ifelse(relative_year < -5, -5, relative_year),
             treated_ever = ifelse(expanded == TRUE, 1, 0))
    result_event[[i]] = feols(uncomp_care~i(relative_year,treated_ever,ref=-1) | provider+year,
                              cluster=~provider, data=data_event)
  }
}

```
```{r event-study-disply, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
modelsummary(result_event,stars=TRUE,output='markdown')
```
\newpage

\blandscape

5. Sun and Abraham (SA) show that the $\delta_{\tau}$ coefficients in Equation \@ref(eq:event) can be written as a non-convex average of all other group-time specific average treatment effects. They propose an interaction weighted specification:
\begin{equation}
y_{it} = \alpha_{i} + \gamma_{t} +\sum_{e} \sum_{\tau \neq -1} \left(D_{it}^{\tau} \times 1(E_{i}=e)\right) \delta_{e, \tau} + \varepsilon_{it}.
\end{equation}
Re-estimate your event study using the SA specification in Equation (3). Show your results for $\hat{\delta}_{e, \tau}$ in a Table, focusing on states with $E_{i}=2014$, $E_{i}=2015$, and $E_{i}=2016$.
```{r sa, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
##### Differential timgin treatment
data_SA <- data_merged %>%
    filter(!is.na(uncomp_care)) %>%
    select(provider,year,state,uncomp_care,expanded,year_adopted) %>%
    mutate(expand_year = ifelse(expanded == FALSE, 10000, as.numeric(year_adopted)),
           time_to_treat = ifelse(expanded == FALSE, -1 ,year-expand_year),
           time_to_treat = ifelse(time_to_treat < -5,-5, time_to_treat)
           )
result_SA <- feols(uncomp_care~sunab(expand_year, time_to_treat, no_agg=TRUE) | provider+year,
                   cluster=~provider, data=data_SA)

list_SA<-list(esttable(result_SA,keep=c("time_to_treat = \\d+ x cohort = 2014","time_to_treat = -\\d+ x cohort = 2014")),
     esttable(result_SA,keep=c("time_to_treat = \\d+ x cohort = 2015","time_to_treat = -\\d+ x cohort = 2015")),
     esttable(result_SA,keep=c("time_to_treat = \\d+ x cohort = 2016","time_to_treat = -\\d+ x cohort = 2016")))

f <- function(tab) {
  newtab <- tab
  rnames <- rownames(tab)
  rownames(newtab) <- str_replace(rnames, " x cohort = \\d+","")
  newtab$key <- rownames(newtab)
  newtab <- as.tibble(newtab)
  return(newtab)
}

SA14 <- f(list_SA[[1]])
SA15 <- f(list_SA[[2]])
SA16 <- f(list_SA[[3]])

table_SA <- SA14 %>%
  left_join(SA15, by='key', suffix=c("14","15")) %>%
  left_join(SA16, by="key", suffix=c("","16"))

rnames <- table_SA$key
table_SA <- table_SA %>% select(!key) %>%
  rename("E = 14" = result_SA14,
         "E = 15" = result_SA15,
         "E = 16" = result_SA)
table_SA$" " <- rnames
table_SA <- table_SA %>% select(" ","E = 14","E = 15","E = 16")
```
```{r sa-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
table_SA %>% kbl()
```

\elandscape

\newpage
6. Present an event study graph based on the results in part 5. Hint: you can do this automatically in `R` with the `fixest` package (using the `sunab` syntax for interactions), or with `eventstudyinteract` in `Stata`. These packages help to avoid mistakes compared to doing the tables/figures manually and also help to get the standard errors correct.

``` {r sa-plot, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
result_SA_agg <- feols(uncomp_care~sunab(expand_year, time_to_treat) | provider+year, cluster=~provider, data=data_SA)
```
```{r sa-plot-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
coefplot(result_SA_agg, main="Aggregate Effect of Medicaid Eaxpansion on Uncompensated Care")
```
\newpage
7. Callaway and Sant'Anna (CS) offer a non-parametric solution that effectively calculates a set of group-time specific differences, $ATT(g,t)= E[y_{it}(g) - y_{it}(\infty) | G_{i}=g]$, where $g$ reflects treatment timing and $t$ denotes time. They show that under the standard DD assumptions of parallel trends and no anticipation, $ATT(g,t) = E[y_{it} - y_{i, g-1} | G_{i}=g] - E[y_{it} - y_{i,g-1} | G_{i} = \infty]$, so that $\hat{ATT}(g,t)$ is directly estimable from sample analogs. CS also propose aggregations of $\hat{ATT}(g,t)$ to form an overall ATT or a time-specific ATT (e.g., ATTs for $\tau$ periods before/after treatment). With this framework in mind, provide an alternative event study using the CS estimator. Hint: check out the `did` package in `R` or the `csdid` package in `Stata`.

```{r cs, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
data_CS <- data_merged %>%
  filter(!is.na(uncomp_care)) %>%
  select(provider,year,state,uncomp_care,expanded,year_adopted) %>%
  mutate(expand_year = ifelse(expanded == FALSE, 0, as.numeric(year_adopted))) %>%
  group_by(provider) %>%
  mutate(provider_id=cur_group_id()) %>% ungroup()


result_CS <- att_gt(yname="uncomp_care", 
                 tname="year", 
                 idname="provider_id",
                 gname="expand_year",
                 data=data_CS, 
                 panel=TRUE, 
                 est_method="dr",
                 allow_unbalanced_panel=TRUE,
                 base_period="universal")

result_CS_event <- aggte(result_CS, type="dynamic", min_e = -5, max_e = 5)

```
```{r cs-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
ggdid(result_CS_event)
```
\newpage
8. Rambachan and Roth (RR) show that traditional tests of parallel pre-trends may be underpowered, and they provide an alternative estimator that essentially bounds the treatment effects by the size of an assumed violation in parallel trends. One such bound RR propose is to limit the post-treatment violation of parallel trends to be no worse than some multiple of the pre-treatment violation of parallel trends. Assuming linear trends, such a relative violation is reflected by 
$$\Delta(\bar{M}) = \left\{ \delta : \forall t \geq 0, \lvert (\delta_{t+1} - \delta_{t}) - (\delta_{t} - \delta_{t-1}) \rvert \leq \bar{M} \times \max_{s<0} \lvert (\delta_{s+1} - \delta_{s}) - (\delta_{s} - \delta_{s-1}) \rvert \right\}.$$
The authors also propose a similar approach with what they call "smoothness restrictions," in which violations in trends changes no more than $M$ between periods. The only difference is that one restriction is imposed relative to observed trends, and one restriction is imposed using specific values. Using the `HonestDiD` package in `R` or `Stata`, present a sensitivity plot of your CS ATT estimates using smoothness restrictions, with assumed violations of size $M \in \left\{ 500, 1000, 1500, 2000 \right\}$. Check out the GitHub repo [here](https://github.com/pedrohcgs/CS_RR) for some help in combining the `HonestDiD` package with CS estimates. Note that you'll need to edit the function in that repo in order to use pre-specified smoothness restrictions. You can do that by simply adding `Mvec=Mvec` in the `createSensitivityResults` function for `type=smoothness`.


```{r Aux-func-RR, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, include=FALSE, warning=FALSE}
honest_did <- function(es, ...) {
  UseMethod("honest_did", es)
}
#' @title honest_did.AGGTEobj
#'
#' @description a function to compute a sensitivity analysis
#'  using the approach of Rambachan and Roth (2021) when
#'  the event study is estimating using the `did` package
#'
#' @param e event time to compute the sensitivity analysis for.
#'  The default value is `e=0` corresponding to the "on impact"
#'  effect of participating in the treatment.
#' @param type Options are "smoothness" (which conducts a
#'  sensitivity analysis allowing for violations of linear trends
#'  in pre-treatment periods) or "relative_magnitude" (which
#'  conducts a sensitivity analysis based on the relative magnitudes
#'  of deviations from parallel trends in pre-treatment periods).
#' @inheritParams HonestDiD::createSensitivityResults
#' @inheritParams HonestDid::createSensitivityResults_relativeMagnitudes
honest_did.AGGTEobj <- function(es,
                                e=0,
                                type=c("smoothness", "relative_magnitude"),
                                method=NULL,
                                bound="deviation from parallel trends",
                                Mvec=NULL,
                                Mbarvec=NULL,
                                monotonicityDirection=NULL,
                                biasDirection=NULL,
                                alpha=0.05,
                                parallel=FALSE,
                                gridPoints=10^3,
                                grid.ub=NA,
                                grid.lb=NA,
                                ...) {
  
  
  type <- type[1]
  
  # make sure that user is passing in an event study
  if (es$type != "dynamic") {
    stop("need to pass in an event study")
  }
  
  # check if used universal base period and warn otherwise
  if (es$DIDparams$base_period != "universal") {
    warning("it is recommended to use a universal base period for honest_did")
  }
  
  # recover influence function for event study estimates
  es_inf_func <- es$inf.function$dynamic.inf.func.e
  
  # recover variance-covariance matrix
  n <- nrow(es_inf_func)
  V <- t(es_inf_func) %*% es_inf_func / (n*n) 
  
  
  nperiods <- nrow(V)
  npre <- sum(1*(es$egt < 0))
  npost <- nperiods - npre
  
  baseVec1 <- basisVector(index=(e+1),size=npost)
  
  orig_ci <- constructOriginalCS(betahat = es$att.egt,
                                 sigma = V, numPrePeriods = npre,
                                 numPostPeriods = npost,
                                 l_vec = baseVec1)
  
  if (type=="relative_magnitude") {
    if (is.null(method)) method <- "C-LF"
    robust_ci <- createSensitivityResults_relativeMagnitudes(betahat = es$att.egt, sigma = V, 
                                                             numPrePeriods = npre, 
                                                             numPostPeriods = npost,
                                                             bound=bound,
                                                             method=method,
                                                             l_vec = baseVec1,
                                                             Mbarvec = Mbarvec,
                                                             monotonicityDirection=monotonicityDirection,
                                                             biasDirection=biasDirection,
                                                             alpha=alpha,
                                                             gridPoints=100,
                                                             grid.lb=-1,
                                                             grid.ub=1,
                                                             parallel=parallel
                                                             )
    
  } else if (type=="smoothness") {
    robust_ci <- createSensitivityResults(betahat = es$att.egt,
                                          sigma = V, 
                                          numPrePeriods = npre, 
                                          numPostPeriods = npost,
                                          method=method,
                                          l_vec = baseVec1,
                                          monotonicityDirection=monotonicityDirection,
                                          biasDirection=biasDirection,
                                          alpha=alpha,
                                          parallel=parallel, 
                                          Mvec=Mvec)
  }
  
  list(robust_ci=robust_ci, orig_ci=orig_ci, type=type)
}
```


```{r honest, tidy=TRUE, echo=TRUE, cache=TRUE, comment=NA, warning=FALSE,include=FALSE}
M_grid = c(500,1000,1500,2000)
sensitivity_results <- honest_did(result_CS_event,
                        type = "smoothness",
                        Mvec = M_grid)
```
```{r honest-display, tidy=TRUE, echo=FALSE, cache=TRUE, comment=NA, warning=FALSE}
HonestDiD::createSensitivityPlot(sensitivity_results$robust_ci,
                                 sensitivity_results$orig_ci)
```

\newpage

9. Discuss your findings and compare estimates from different estimators (e.g., are your results sensitive to different specifications or estimators? Are your results sensitive to violation of parallel trends assumptions?).

Answers: Simple DiD estimation result indicate that the treatment effect is significant and robust to different treatment timing. The treatment effect is still robust when estimating event study. Sun and Abraham (SA) and Callaway and Sant'Anna (CS) also shows stable treatment effects. Rambachan and Roth (RR) method indicates that the results are robust to parallel trend assumption.


10. Reflect on this assignment. What did you find most challenging? What did you find most surprising? 

Answers: Understanding and cleaning datasets are most challenging. I was surprised that there are multiple methods to do DiD and sensitivity analysis. This assignmet is really helpful to understand DiD deeper.
